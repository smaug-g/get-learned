{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "TRAIN_CSV_NAME = 'train_data.csv'\n",
    "TEST_CSV_NAME = 'test_data.csv'\n",
    "TRAIN_IMG_DIRECTORY = 'train_posters'\n",
    "TEST_IMG_DIRECTORY = 'test_posters'\n",
    "SEED = 11\n",
    "np.random.seed(SEED) # Seed our randomness for reproducibilty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#First load and preprocess the TFIDF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bg.build_tfidf import preprocess_nonimage_data\n",
    "Xtfidf, Xother, y, vectorizer = preprocess_nonimage_data(TRAIN_CSV_NAME)\n",
    "# Choose indices for our test data\n",
    "testIdx = np.random.choice(y.shape[0], 200, replace=False)\n",
    "X = np.zeros([Xtfidf.shape[0], Xtfidf.shape[1] + 2])\n",
    "X[:,0:2], X[:, 2:]  = Xother[:, 2:], Xtfidf\n",
    "\n",
    "X_test = X[testIdx, :]\n",
    "X_train = np.delete(X, testIdx, axis=0)\n",
    "y_test = y[testIdx]\n",
    "y_train = np.delete(y, testIdx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train an SVM on the TFIDF data vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CalibratedClassifierCV(base_estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n     intercept_scaling=1, loss='squared_hinge', max_iter=300000,\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n     verbose=0),\n            cv=3, method='sigmoid')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "clf = CalibratedClassifierCV(svm.LinearSVC(penalty='l2', dual=True, multi_class='ovr', max_iter=100000),\n",
    "                                        method='sigmoid',\n",
    "                                        cv=3)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification rate 0.620000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = clf.predict(X_test)\n",
    "a = confusion_matrix(y_test, y_pred)\n",
    "print(\"Classification rate %f\" %(a.trace() / a.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Now load and preprocess the color histogram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bg.util import build_histograms, preprocess_images\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "train_data_csv = pd.read_csv(TRAIN_CSV_NAME).values\n",
    "genres = np.array(train_data_csv[:,-1]).astype(int)\n",
    "test_data_csv = pd.read_csv(TEST_CSV_NAME).values\n",
    "preprocessed_train = preprocess_images(train_data_csv, TRAIN_IMG_DIRECTORY)\n",
    "histogram_processed_train = build_histograms(preprocessed_train, 12)\n",
    "\n",
    "bgX_test = histogram_processed_train[testIdx]\n",
    "bgX_train = np.delete(histogram_processed_train, testIdx, axis=0)\n",
    "bgy_test = genres[testIdx]\n",
    "bgy_train = np.delete(genres, testIdx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now fit/test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.495\n0.495\n"
     ]
    }
   ],
   "source": [
    "bg_model = RandomForestClassifier(n_estimators=800, random_state=SEED)\n",
    "bg_model.fit(bgX_train, bgy_train)\n",
    "print(bg_model.score(bgX_test, bgy_test))\n",
    "print(bg_model.score(bgX_test, y_test)) # This proves that we are using the same labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally let's try an ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification rate 0.710000\n"
     ]
    }
   ],
   "source": [
    "from bg.util import trainModelStack, predictModelStack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "combiner = LogisticRegression() # Standard Logistic classifier. Worth trying some other things.\n",
    "models = [bg_model, clf]\n",
    "datas = [bgX_train, X_train]\n",
    "trainModelStack(models, combiner, datas, y_train)\n",
    "predictions = predictModelStack(models, combiner, [bgX_test, X_test])\n",
    "cm = confusion_matrix(predictions, y_pred)\n",
    "print(\"Classification rate %f\" %(cm.trace() / cm.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's make a Kaggle submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(343, 3841)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(343, 3841) (2894, 3843)\n(343, 1728)\n(343, 3843)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(343,)\n"
     ]
    }
   ],
   "source": [
    "# del preprocess_nonimage_data\n",
    "from bg.build_tfidf import preprocess_nonimage_data\n",
    "tfTest, otherTest, _, _ = preprocess_nonimage_data(TEST_CSV_NAME, True, csvSize=343, vectorizer=vectorizer)\n",
    "print(tfTest.shape)\n",
    "testXTf = np.zeros([tfTest.shape[0], tfTest.shape[1] + 2])\n",
    "testXTf[:,0:2], testXTf[:, 2:]  = otherTest[:, 2:], tfTest\n",
    "preprocessed_test = preprocess_images(test_data_csv, TEST_IMG_DIRECTORY)\n",
    "histogram_processed_test = build_histograms(preprocessed_test, 12)\n",
    "\n",
    "print(tfTest.shape, X_train.shape)\n",
    "print(histogram_processed_test.shape)\n",
    "print(testXTf.shape)\n",
    "test_predictions_tfidf = clf.predict(testXTf)\n",
    "test_predictions_hist = bg_model.predict(histogram_processed_test)\n",
    "test_predictions = predictModelStack(models, combiner, [histogram_processed_test, testXTf])\n",
    "print(test_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bg.util import predictions_to_csv\n",
    "predictions_to_csv(test_predictions, \"stacked.csv\") # Not working for some reason so I'm doing it manually below\n",
    "labels = ['Id', 'Category']\n",
    "df = pd.DataFrame.from_records(enumerate(test_predictions), columns=labels)\n",
    "df.to_csv('stacked.csv', mode='w', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
