{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "TRAIN_CSV_NAME = 'train_data.csv'\n",
    "TEST_CSV_NAME = 'test_data.csv'\n",
    "TRAIN_IMG_DIRECTORY = 'train_posters'\n",
    "TEST_IMG_DIRECTORY = 'test_posters'\n",
    "TEST_SIZE = 200\n",
    "SEED = 1\n",
    "np.random.seed(SEED) # Seed our randomness for reproducibilty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#First load and preprocess the TFIDF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bg.build_tfidf import preprocess_nonimage_data\n",
    "Xtfidf, Xother, y, vectorizer = preprocess_nonimage_data(TRAIN_CSV_NAME)\n",
    "# Choose indices for our test data\n",
    "testIdx = np.random.choice(y.shape[0], TEST_SIZE, replace=False)\n",
    "X = np.zeros([Xtfidf.shape[0], Xtfidf.shape[1] + 2])\n",
    "X[:,0:2], X[:, 2:]  = Xother[:, 2:], Xtfidf\n",
    "\n",
    "X_test = X[testIdx, :]\n",
    "X_train = np.delete(X, testIdx, axis=0)\n",
    "y_test = y[testIdx]\n",
    "y_train = np.delete(y, testIdx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train an SVM on the TFIDF data vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[282  77 155  98]\n [ 64 250 108 135]\n [108 122 248 281]\n [ 41 105  54 766]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.52 (+/- 0.01) (95 percent confidence)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CalibratedClassifierCV(base_estimator=LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,\n     intercept_scaling=1, loss='squared_hinge', max_iter=100000,\n     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n     verbose=0),\n            cv=5, method='sigmoid')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = CalibratedClassifierCV(svm.LinearSVC(penalty='l2', dual=True, multi_class='ovr', max_iter=100000, class_weight='balanced'),\n",
    "                                        cv=5)\n",
    "y_predTF = cross_val_predict(clf, X_train, y_train, cv=5)\n",
    "conf_mat = confusion_matrix(y_train, y_predTF)\n",
    "print(conf_mat)\n",
    "scoresTF = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f) (95 percent confidence)\" % (scores.mean(), scores.std() * 2))\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification rate 0.585000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = clf.predict(X_test)\n",
    "a = confusion_matrix(y_test, y_pred)\n",
    "print(\"Classification rate %f\" %(a.trace() / a.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Now load and preprocess the color histogram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bg.util import build_histograms, preprocess_images, extract_texture_features\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "train_data_csv = pd.read_csv(TRAIN_CSV_NAME).values\n",
    "genres = np.array(train_data_csv[:,-1]).astype(int)\n",
    "test_data_csv = pd.read_csv(TEST_CSV_NAME).values\n",
    "preprocessed_train = preprocess_images(train_data_csv, TRAIN_IMG_DIRECTORY)\n",
    "histogram_processed_train = build_histograms(preprocessed_train, 12)\n",
    "# texture_processed_train = np.array(list(map(extract_texture_features, preprocessed_train)))\n",
    "\n",
    "bgX_test = histogram_processed_train[testIdx]\n",
    "bgX_train = np.delete(histogram_processed_train, testIdx, axis=0)\n",
    "\n",
    "# bgX2_test = texture_processed_train[testIdx]\n",
    "# bgX2_train = np.delete(texture_processed_train, testIdx, axis=0)\n",
    "\n",
    "bgy_test = genres[testIdx]\n",
    "bgy_train = np.delete(genres, testIdx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now fit/test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[267  54 146 145]\n [ 39 353  47 118]\n [125  77 273 284]\n [ 82 113 154 617]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "bg_model = RandomForestClassifier(n_estimators=400, random_state=SEED)\n",
    "y_pred = cross_val_predict(bg_model, bgX_train, bgy_train, cv=5)\n",
    "conf_mat = confusion_matrix(bgy_train, y_pred)\n",
    "print(conf_mat)\n",
    "scores = cross_val_score(bg_model, bgX_train, bgy_train)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f) (95 percent confidence)\" % (scores.mean(), scores.std() * 2))\n",
    "bg_model.fit(bgX_train, bgy_train)\n",
    "print(bg_model.score(bgX_test, bgy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally let's try an ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification rate 0.650000\n"
     ]
    }
   ],
   "source": [
    "from bg.util import trainModelStack, predictModelStack\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "combiner = LogisticRegressionCV()  # Standard Logistic classifier. Worth trying some other things.\n",
    "models = [clf, bg_model]\n",
    "datas = [X_train, bgX_train]\n",
    "trainModelStack(models, combiner, datas, y_train)\n",
    "predictions = predictModelStack(models, combiner, [X_test, bgX_test])\n",
    "cm = confusion_matrix(predictions, y_pred)\n",
    "print(\"Classification rate %f\" %(cm.trace() / cm.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's make a Kaggle submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(343, 3841)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/anaconda3/lib/python3.7/site-packages/skimage/util/dtype.py:130: UserWarning: Possible precision loss when converting from float64 to uint8\n  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(343, 3841) (3092, 3843)\n(343, 1728)\n(343, 3843)\n(343,)\n"
     ]
    }
   ],
   "source": [
    "# del preprocess_nonimage_data\n",
    "from bg.build_tfidf import preprocess_nonimage_data\n",
    "tfTest, otherTest, _, _ = preprocess_nonimage_data(TEST_CSV_NAME, True, csvSize=343, vectorizer=vectorizer)\n",
    "print(tfTest.shape)\n",
    "testXTf = np.zeros([tfTest.shape[0], tfTest.shape[1] + 2])\n",
    "testXTf[:,0:2], testXTf[:, 2:]  = otherTest[:, 2:], tfTest\n",
    "preprocessed_test = preprocess_images(test_data_csv, TEST_IMG_DIRECTORY)\n",
    "histogram_processed_test = build_histograms(preprocessed_test, 12)\n",
    "texture_processed_test = np.array(list(map(extract_texture_features, preprocessed_test)))\n",
    "\n",
    "print(tfTest.shape, X_train.shape)\n",
    "print(histogram_processed_test.shape)\n",
    "print(testXTf.shape)\n",
    "# test_predictions_tfidf = clf.predict(testXTf)\n",
    "test_predictions_hist = bg_model.predict(histogram_processed_test)\n",
    "test_predictions = predictModelStack(models, combiner, [testXTf, histogram_processed_test, texture_processed_test])\n",
    "print(test_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bg.util import predictions_to_csv\n",
    "predictions_to_csv(test_predictions, \"stacked7.csv\") # Not working for some reason so I'm doing it manually below\n",
    "labels = ['Id', 'Category']\n",
    "df = pd.DataFrame.from_records(enumerate(test_predictions), columns=labels)\n",
    "df.to_csv('stacked7.csv', mode='w', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
